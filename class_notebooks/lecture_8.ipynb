{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Advanced Machine Learning (Semester 1 2023)**\n",
    "# 8 Reinforcement Learning\n",
    "\n",
    "\n",
    "*N. Hernitschek, 2023*\n",
    "\n",
    "\n",
    "This Jupyter notebook gives an intro to Reinforcement Learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Contents\n",
    "* [Reinforcement Learning with `keras`/`tensorflow`](#first-bullet)\n",
    "* [Water Flow Control](#second-bullet)\n",
    "* [Summary](#third-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reinforcement Learning with `keras`/ `tensorflow` <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "Machine learning methods we have seen so far either fall into the category of supervised or unsupervised algorithms.\n",
    "Reinforcement Learning stands out because it is used to train models in a live environment.\n",
    "\n",
    "\n",
    "`keras-rl` implements some state-of-the art deep reinforcement learning algorithms in Python and seamlessly integrates with the Deep Learning library `keras`.\n",
    "\n",
    "One can extend `keras-rl` according to their own needs by e.g. building metrics and callbacks in addition to the built-in ones. Even more so, it is easy to implement your own environments and even algorithms by simply extending some simple abstract classes. \n",
    "You can find more information at \n",
    "https://github.com/keras-rl/keras-rl\n",
    "\n",
    "OpenAI `gym` is an open-source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, this API has become the field standard.\n",
    "Furthermore, `keras-rl` works with OpenAI Gym out of the box. This means that evaluating and playing around with different algorithms is easy.\n",
    "You can find more information at \n",
    "https://github.com/openai/gym\n",
    "\n",
    "\n",
    "\n",
    "OpenAI `gym` comes with standard test environments for Reinforcement Learning, such as simple computer games like \"Space Invaders\", or simulations like balancing a pole on a moving cart. Whereas such environments can be fun to try out and can give an idea on the performance of various Reinforcement Learning algorithms, this can be limiting.\n",
    "For this reason, this tutorial will show you how to build a custom Reinforcement Learning environment using OpenAI `gym`. Specifically, we will build an Reinforcement Learning model to automatically regulate temperature and get it to an optimal range.\n",
    "\n",
    "\n",
    "First we install the libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting keras-rl2\n",
      "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m970.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow in /home/nhernits/.local/lib/python3.10/site-packages (from keras-rl2) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.32.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (4.22.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (16.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow->keras-rl2) (59.6.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.2.0)\n",
      "Requirement already satisfied: packaging in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (23.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (3.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.53.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (23.3.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow->keras-rl2) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.12.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.4.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.23.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /home/nhernits/.local/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-rl2) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /home/nhernits/.local/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-rl2) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.17.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.2.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/nhernits/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.28.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/nhernits/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/nhernits/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/nhernits/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/nhernits/.local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nhernits/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nhernits/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nhernits/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nhernits/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/nhernits/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/nhernits/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.2.0)\n",
      "Installing collected packages: keras-rl2\n",
      "Successfully installed keras-rl2-1.0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Keras-rl2 gives us several pre-defined agents to build Reinforcement Learning models.\n",
    "#!pip install keras-rl2\n",
    "\n",
    "#OpenAI gym provides environments for Reinforcement Learning\n",
    "#!pip install gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Application: Water Flow Control <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "We want to build a Reinforcement Learning model to automatically regulate temperature and get it to an optimal range.\n",
    "\n",
    "\n",
    "**Goals:**\n",
    "    \n",
    "1. we want our optimal temperature to be between 37 and 39 degrees Celcius.\n",
    "\n",
    "2. The shower length will be 60 seconds. This means that the **episode length** will be 60 seconds in which the model will try to get into that optimal temperature range within 60 seconds.\n",
    "\n",
    "3. Our model will perform three actions: turn up, keep, and turn down the temperature. \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The placeholder class `Env` from OpenAI `gym` allows us to build our custom environment.\n",
    "\n",
    "The `Discrete` and `Box` spaces from `gym.spaces` allow us to define the actions and the current state we can take on our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Building the custom RL environment with OpenAI Gym\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We begin by creating a `CustomEnv` class. By passing `Env` to the `CustomEnv` class, we **inherit** the methods and properties from the OpenAI `gym` environment class.\n",
    "\n",
    "Within the `CustomEnv`class, we implement the `__init__` function to initialize the actions, observations, and episode length.\n",
    "The actions are: down (`0`), keep(`1`), up (`2`).\n",
    "\n",
    "The `observation_space` will hold an array of our current temperature. Next, we set our start temperature to 38 degrees plus a random integer. Finally, we’ve set the shower length to 60 seconds.\n",
    "\n",
    "Other than `Discrete` spaces, `Box` spaces are much more flexible and allow us to pass through multiple values between 0 and 100. In addition, they can be used to hold other data such as images, audio, and data frames.\n",
    "\n",
    "The `step` function defines what we do after we take action. We’ve set our action value to `-1`. Ideally, this means that:\n",
    "\n",
    " *   If we apply action `0` together with `-1`, we get a `-1` value. This action will lower the temperature by 1.\n",
    " *   If we apply action `1` together with `-1`, we get a `0` value. This action will maintain the current temperature.\n",
    " *   If we apply action `2` together with `-1`, we get a `1` value. This action will increase the temperature by 1.\n",
    "\n",
    "Each step, We are also reducing the remaining shower length by 1.\n",
    "\n",
    "When calculating the **reward**:\n",
    "\n",
    "* If the temperature is in its optimal range of 37, and 39, we give a reward of 1.\n",
    "* If the temperature is not in the optimal range, we give a reward of `-1`. \n",
    "\n",
    "Our model will always try to converge with this function so that the temperature is within the optimal range.\n",
    "\n",
    "\n",
    "We use the `reset` function to reset our environment or update each episode. It resets the shower temperature and time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        self.shower_length = 60\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.state += action -1 \n",
    "        self.shower_length -= 1 \n",
    "        \n",
    "        if self.state >=37 and self.state <=39: \n",
    "            reward =1 \n",
    "        else: \n",
    "            reward = -1 \n",
    "        \n",
    "        if self.shower_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        self.shower_length = 60 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([84.878075], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s play around with our environment without doing any training. We're just sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:10\n",
      "Episode:2 Score:8\n",
      "Episode:3 Score:2\n",
      "Episode:4 Score:-38\n",
      "Episode:5 Score:-60\n",
      "Episode:6 Score:-60\n",
      "Episode:7 Score:-42\n",
      "Episode:8 Score:-48\n",
      "Episode:9 Score:-10\n",
      "Episode:10 Score:8\n",
      "Episode:11 Score:-10\n",
      "Episode:12 Score:4\n",
      "Episode:13 Score:-60\n",
      "Episode:14 Score:-46\n",
      "Episode:15 Score:-24\n",
      "Episode:16 Score:-38\n",
      "Episode:17 Score:-56\n",
      "Episode:18 Score:-50\n",
      "Episode:19 Score:-30\n",
      "Episode:20 Score:30\n"
     ]
    }
   ],
   "source": [
    "episodes = 20 #20 shower episodes\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running through 20 different showers, we get different reward values. \n",
    "Remember, if the shower is not within the optimal range of between 37 and 39 degrees, we get a reward of `-1`.\n",
    "\n",
    "Most of the rewards indicate that we were way outside our optimal temperature range.\n",
    "The best reward typically between 25 and 30, which indicates that some of the steps that we took may have been within that optimal range.\n",
    "\n",
    "Let’s go ahead and use `keras` to build a **Deep Learning model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Creating a Deep Learning model using Keras\n",
    "\n",
    "\n",
    "The first step involves defining our states and actions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model, we are passing in the temperature (`input_shape=states`) to the input of our Deep Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 24)                48        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 723\n",
      "Trainable params: 723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Building the agent with`keras-RL`\n",
    "\n",
    "We can then pass this model to the `Keras-RL` model.\n",
    "\n",
    "We use the Boltzmann Q Policy. It builds a probability law on q values and returns an action selected randomly according to this law.\n",
    "\n",
    "\n",
    "We build a `DQNagent` using the model we created in the section above.\n",
    "A DQN agent is a value-based reinforcement learning agent that trains a critic to estimate the return or future rewards. DQN is a variant of Q-learning. The DQN agent uses the Sequential memory to store various states, actions, and rewards.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom Reinforcement Learning environment can now train our `dqn` model to set the correct optimal temperature.\n",
    "\n",
    "We train the agent for 60000 steps, but you could train it for longer to produce better results by adjusting the `nb_steps` parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 15:27:07.671126: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_10_2/kernel/Assign' id:1004 op device:{requested: '', assigned: ''} def:{{{node dense_10_2/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_10_2/kernel, dense_10_2/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 60000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "\r",
      "    1/10000 [..............................] - ETA: 17:59 - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhernits/.local/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-06-07 15:27:08.025049: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_11/BiasAdd' id:775 op device:{requested: '', assigned: ''} def:{{{node dense_11/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_11/MatMul, dense_11/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-06-07 15:27:08.060000: W tensorflow/c/c_api.cc:300] Operation '{name:'total_3/Assign' id:1191 op device:{requested: '', assigned: ''} def:{{{node total_3/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_3, total_3/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/nhernits/.local/lib/python3.10/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "2023-06-07 15:27:08.152530: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_11_2/BiasAdd' id:1038 op device:{requested: '', assigned: ''} def:{{{node dense_11_2/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_11_2/MatMul, dense_11_2/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-06-07 15:27:08.406366: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_9/AddN' id:1292 op device:{requested: '', assigned: ''} def:{{{node loss_9/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_9/mul, loss_9/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-06-07 15:27:08.458803: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/learning_rate/Assign' id:1449 op device:{requested: '', assigned: ''} def:{{{node training/Adam/learning_rate/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/learning_rate, training/Adam/learning_rate/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.4822\n",
      "166 episodes - episode_reward: -29.000 [-60.000, 40.000] - loss: 0.801 - mae: 4.947 - mean_q: -5.620\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 82s 8ms/step - reward: -0.5052\n",
      "167 episodes - episode_reward: -30.180 [-60.000, 36.000] - loss: 1.296 - mae: 7.799 - mean_q: -10.993\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.5064\n",
      "167 episodes - episode_reward: -30.443 [-60.000, 30.000] - loss: 2.104 - mae: 10.180 - mean_q: -14.573\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 90s 9ms/step - reward: -0.3902\n",
      "166 episodes - episode_reward: -23.434 [-60.000, 34.000] - loss: 1.983 - mae: 9.731 - mean_q: -13.899\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -0.4044\n",
      "167 episodes - episode_reward: -24.168 [-60.000, 34.000] - loss: 1.825 - mae: 9.315 - mean_q: -13.270\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -0.1968\n",
      "done, took 495.196 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0c4b063430>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "\n",
    "dqn.compile(tf.keras.optimizers.legacy.Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "dqn.fit(env, nb_steps=60000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After 60000 steps, we get a reward of typically between -0.1 and -0.3. In the initial 10000 steps, we begin with a reward of -0.6412. This decreased to -0.3908 at the end.\n",
    "\n",
    "The model is not perfect but when we increase the number of training steps, you will get better results (positive rewards).\n",
    "\n",
    "Positive values mean that the temperature is within its optimal temperature. You can try adding some random figures when creating the model and see how your agent will behave after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Testing our custom Reinforcement Learning environment\n",
    "\n",
    "After training our model, we can test it out. \n",
    "\n",
    "This is an ideal example and might not represent a real-case scenario, i.e., when something else is influencing with the temperature. It is thus always important to build a model as close as possible to the scenario in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 150 episodes ...\n",
      "Episode 1: reward: 60.000, steps: 60\n",
      "Episode 2: reward: 60.000, steps: 60\n",
      "Episode 3: reward: 60.000, steps: 60\n",
      "Episode 4: reward: 60.000, steps: 60\n",
      "Episode 5: reward: 60.000, steps: 60\n",
      "Episode 6: reward: 60.000, steps: 60\n",
      "Episode 7: reward: 60.000, steps: 60\n",
      "Episode 8: reward: 60.000, steps: 60\n",
      "Episode 9: reward: 60.000, steps: 60\n",
      "Episode 10: reward: 60.000, steps: 60\n",
      "Episode 11: reward: 60.000, steps: 60\n",
      "Episode 12: reward: 58.000, steps: 60\n",
      "Episode 13: reward: 60.000, steps: 60\n",
      "Episode 14: reward: 60.000, steps: 60\n",
      "Episode 15: reward: 60.000, steps: 60\n",
      "Episode 16: reward: 60.000, steps: 60\n",
      "Episode 17: reward: 60.000, steps: 60\n",
      "Episode 18: reward: 58.000, steps: 60\n",
      "Episode 19: reward: 60.000, steps: 60\n",
      "Episode 20: reward: 58.000, steps: 60\n",
      "Episode 21: reward: 60.000, steps: 60\n",
      "Episode 22: reward: 60.000, steps: 60\n",
      "Episode 23: reward: 60.000, steps: 60\n",
      "Episode 24: reward: 60.000, steps: 60\n",
      "Episode 25: reward: 60.000, steps: 60\n",
      "Episode 26: reward: 60.000, steps: 60\n",
      "Episode 27: reward: 60.000, steps: 60\n",
      "Episode 28: reward: 60.000, steps: 60\n",
      "Episode 29: reward: 60.000, steps: 60\n",
      "Episode 30: reward: 60.000, steps: 60\n",
      "Episode 31: reward: 60.000, steps: 60\n",
      "Episode 32: reward: 60.000, steps: 60\n",
      "Episode 33: reward: 60.000, steps: 60\n",
      "Episode 34: reward: 58.000, steps: 60\n",
      "Episode 35: reward: 60.000, steps: 60\n",
      "Episode 36: reward: 58.000, steps: 60\n",
      "Episode 37: reward: 60.000, steps: 60\n",
      "Episode 38: reward: 60.000, steps: 60\n",
      "Episode 39: reward: 60.000, steps: 60\n",
      "Episode 40: reward: 58.000, steps: 60\n",
      "Episode 41: reward: 60.000, steps: 60\n",
      "Episode 42: reward: 58.000, steps: 60\n",
      "Episode 43: reward: 60.000, steps: 60\n",
      "Episode 44: reward: 60.000, steps: 60\n",
      "Episode 45: reward: 60.000, steps: 60\n",
      "Episode 46: reward: 58.000, steps: 60\n",
      "Episode 47: reward: 60.000, steps: 60\n",
      "Episode 48: reward: 58.000, steps: 60\n",
      "Episode 49: reward: 60.000, steps: 60\n",
      "Episode 50: reward: 60.000, steps: 60\n",
      "Episode 51: reward: 60.000, steps: 60\n",
      "Episode 52: reward: 60.000, steps: 60\n",
      "Episode 53: reward: 60.000, steps: 60\n",
      "Episode 54: reward: 58.000, steps: 60\n",
      "Episode 55: reward: 60.000, steps: 60\n",
      "Episode 56: reward: 60.000, steps: 60\n",
      "Episode 57: reward: 60.000, steps: 60\n",
      "Episode 58: reward: 60.000, steps: 60\n",
      "Episode 59: reward: 60.000, steps: 60\n",
      "Episode 60: reward: 60.000, steps: 60\n",
      "Episode 61: reward: 58.000, steps: 60\n",
      "Episode 62: reward: 60.000, steps: 60\n",
      "Episode 63: reward: 60.000, steps: 60\n",
      "Episode 64: reward: 58.000, steps: 60\n",
      "Episode 65: reward: 60.000, steps: 60\n",
      "Episode 66: reward: 60.000, steps: 60\n",
      "Episode 67: reward: 60.000, steps: 60\n",
      "Episode 68: reward: 60.000, steps: 60\n",
      "Episode 69: reward: 60.000, steps: 60\n",
      "Episode 70: reward: 60.000, steps: 60\n",
      "Episode 71: reward: 60.000, steps: 60\n",
      "Episode 72: reward: 60.000, steps: 60\n",
      "Episode 73: reward: 60.000, steps: 60\n",
      "Episode 74: reward: 60.000, steps: 60\n",
      "Episode 75: reward: 60.000, steps: 60\n",
      "Episode 76: reward: 60.000, steps: 60\n",
      "Episode 77: reward: 58.000, steps: 60\n",
      "Episode 78: reward: 58.000, steps: 60\n",
      "Episode 79: reward: 60.000, steps: 60\n",
      "Episode 80: reward: 60.000, steps: 60\n",
      "Episode 81: reward: 60.000, steps: 60\n",
      "Episode 82: reward: 60.000, steps: 60\n",
      "Episode 83: reward: 60.000, steps: 60\n",
      "Episode 84: reward: 60.000, steps: 60\n",
      "Episode 85: reward: 60.000, steps: 60\n",
      "Episode 86: reward: 60.000, steps: 60\n",
      "Episode 87: reward: 60.000, steps: 60\n",
      "Episode 88: reward: 60.000, steps: 60\n",
      "Episode 89: reward: 60.000, steps: 60\n",
      "Episode 90: reward: 60.000, steps: 60\n",
      "Episode 91: reward: 58.000, steps: 60\n",
      "Episode 92: reward: 58.000, steps: 60\n",
      "Episode 93: reward: 58.000, steps: 60\n",
      "Episode 94: reward: 60.000, steps: 60\n",
      "Episode 95: reward: 60.000, steps: 60\n",
      "Episode 96: reward: 60.000, steps: 60\n",
      "Episode 97: reward: 60.000, steps: 60\n",
      "Episode 98: reward: 60.000, steps: 60\n",
      "Episode 99: reward: 60.000, steps: 60\n",
      "Episode 100: reward: 60.000, steps: 60\n",
      "Episode 101: reward: 60.000, steps: 60\n",
      "Episode 102: reward: 60.000, steps: 60\n",
      "Episode 103: reward: 60.000, steps: 60\n",
      "Episode 104: reward: 60.000, steps: 60\n",
      "Episode 105: reward: 60.000, steps: 60\n",
      "Episode 106: reward: 60.000, steps: 60\n",
      "Episode 107: reward: 60.000, steps: 60\n",
      "Episode 108: reward: 60.000, steps: 60\n",
      "Episode 109: reward: 60.000, steps: 60\n",
      "Episode 110: reward: 60.000, steps: 60\n",
      "Episode 111: reward: 60.000, steps: 60\n",
      "Episode 112: reward: 60.000, steps: 60\n",
      "Episode 113: reward: 58.000, steps: 60\n",
      "Episode 114: reward: 60.000, steps: 60\n",
      "Episode 115: reward: 60.000, steps: 60\n",
      "Episode 116: reward: 58.000, steps: 60\n",
      "Episode 117: reward: 60.000, steps: 60\n",
      "Episode 118: reward: 58.000, steps: 60\n",
      "Episode 119: reward: 60.000, steps: 60\n",
      "Episode 120: reward: 60.000, steps: 60\n",
      "Episode 121: reward: 58.000, steps: 60\n",
      "Episode 122: reward: 60.000, steps: 60\n",
      "Episode 123: reward: 60.000, steps: 60\n",
      "Episode 124: reward: 58.000, steps: 60\n",
      "Episode 125: reward: 60.000, steps: 60\n",
      "Episode 126: reward: 58.000, steps: 60\n",
      "Episode 127: reward: 58.000, steps: 60\n",
      "Episode 128: reward: 60.000, steps: 60\n",
      "Episode 129: reward: 60.000, steps: 60\n",
      "Episode 130: reward: 60.000, steps: 60\n",
      "Episode 131: reward: 60.000, steps: 60\n",
      "Episode 132: reward: 60.000, steps: 60\n",
      "Episode 133: reward: 60.000, steps: 60\n",
      "Episode 134: reward: 60.000, steps: 60\n",
      "Episode 135: reward: 60.000, steps: 60\n",
      "Episode 136: reward: 58.000, steps: 60\n",
      "Episode 137: reward: 58.000, steps: 60\n",
      "Episode 138: reward: 60.000, steps: 60\n",
      "Episode 139: reward: 58.000, steps: 60\n",
      "Episode 140: reward: 60.000, steps: 60\n",
      "Episode 141: reward: 60.000, steps: 60\n",
      "Episode 142: reward: 60.000, steps: 60\n",
      "Episode 143: reward: 60.000, steps: 60\n",
      "Episode 144: reward: 60.000, steps: 60\n",
      "Episode 145: reward: 60.000, steps: 60\n",
      "Episode 146: reward: 60.000, steps: 60\n",
      "Episode 147: reward: 60.000, steps: 60\n",
      "Episode 148: reward: 58.000, steps: 60\n",
      "Episode 149: reward: 58.000, steps: 60\n",
      "Episode 150: reward: 60.000, steps: 60\n",
      "59.61333333333334\n"
     ]
    }
   ],
   "source": [
    "results = dqn.test(env, nb_episodes=150, visualize=False)\n",
    "print(np.mean(results.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n",
    "At this point, all of you should have:\n",
    "* seen how to use Reinforcement Learning with `keras`\n",
    "* seen how to build custom Environments with `keras`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
